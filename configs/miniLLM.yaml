model:
  name: "MiniLLM"
  attention:
    name: "GQAttention"
    max_seq_len: 512
    hidden_size: 512
    num_attention_heads: 16
    num_key_value_heads: 8
    flash_attn: True
    attention_bias: False
    attention_dropout: 0.3

  FFN:
    name: "MLP"
    hidden_size: 512
    intermediate_size: 2048
    mlp_bias: False
    mlp_dropout: 0.3

dataset:
  tokenizer:
    path: "./tokenizer"
  
  data_path: "./dataset/pretrain_hq.jsonl"
  max_length: 512

train:
  optimizer:
    name: AdamW
  batch_size: 4
  num_workers: 4

  lr_scheduler:
    name: "lr"

output_dir: "./output"
